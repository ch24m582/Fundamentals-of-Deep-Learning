<!-- chapter1.html -->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Basics Needed for Deep Learning</title>

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="../js/math-config.js"></script>
</head>

<body>
    <div class="container">
        <a href="../index.html" class="back-link">‚Üê Back to Home</a>

        <header>
            <h1>Chapter 1: Basics Needed for Deep Learning</h1>
        </header>

        <div class="chapter-intro">
            <p>
                \[
                y = h(x) = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2
                \]
                <img src="../assets/LinearForm.png" alt="Linear Regression Illustration" class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy"-->
                - Guess the form of the hypothesis function for a problem with N features.<br>
                - here the form is linear combination of the features and their interactions.<br>
                - we will fix the form of the hypothesis function and learn the parameters (weights) from data.
                
            </p>
        </div>

        <div class="Gradient-Descent">
            <div class="Gradient-Descent">
                <h2>Gradient-Descent</h2>

                <div class="Gradient-Descent Algorithm">
                    <h4>Gradient Descent Update Rule</h4>
                    <p>
                    \[
                    w^{(t+1)} = w^t - \alpha \nabla_w J(w^t)
                    \]
                    <h4>Stoppping criteria:</h4>
                    \[
                    ||w^{(t+1)} - w^t|| &equiv; || \nabla_w J(w^t) || &equiv; || J(W^{(t+1)} - J(w^t)) || < \epsilon
                    \]
                    </p>
                </div>

                <div class ="Gradient Descent for Linear Regression">
                    <h4>Gradient Descent for Linear Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x$ &nbsp;&nbsp;  (We always assume m as total samples)
                    </p>
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(\hat{y}^{(i)} - y^{(i)}\big)^2\]
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(w_0 + w_1x^{(i)} - y^{(i)}\big)^2\]
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} \]
                        \[w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} \]
                    
                    </p>

                    <h4>Gradient Descent for Quadratic Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x + w_2 x^2$ 
                    </p>
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[\frac{\partial J}{\partial w_2} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).{x^{(i)}}^2\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_2 = w_2 - \alpha\frac{\partial J}{\partial w_2} \]
                    
                    </p>   
                    <h4>How a typical data matrix and y looks like</h4>
                    <p> 
                        \[
                        \begin{bmatrix}
                        1 & x_1^{(1)} & x_2^{(1)} & \ldots & x_n^1 \\
                        1 & x_1^{(2)} & x_2^{(2)} & \ldots & x_n^2 \\
                        \vdots & \vdots  & \ddots & \vdots \\
                        1 & x_1^{(m)} & x_2^{(m)} & \ldots & x_n^m 
                        \end{bmatrix}
                        \begin{bmatrix}
                        w_0 \\
                        w_1 \\
                        \vdots \\
                        w_n
                        \end{bmatrix}
                        =
                        \begin{bmatrix}
                        y^{(1)} \\
                        y^{(2)} \\
                        \vdots \\
                        y^{(m)}
                        \end{bmatrix}
                        \]
                    </p>
                    <p style="text-align:center;"> 
                      A typical input can be as weird as shown below:
                      \[x_i^m = sin(\omega x_j^m) &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = log(\omega x_j^m)  &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = \exp\left(-\frac{(x_j^{(m)} - \mu_j)^2}{2\sigma_j^2}\right) \]
                      Weights will remain linear but the input can be as weired 
                    </p>


                </div>

                <div class="note">
                    <h4>Notes</h4>
                    <p>
                        \[ J(w) =\frac{1}{2m} ||XW-Y ||^2 \]
                        Let us say
                        <p style="text-align:center;">$R = C^{\mathrm{T}} W = [\,c_1 \; c_2 \; \cdots \; c_n\,]
                        \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$</p>
                        where \(C\) is a constant vector independent of \(W\). Then,
                        \[
                        \frac{\partial R}{\partial w}
                        = \begin{bmatrix}
                        \frac{\partial R}{\partial w_1} \\
                        \frac{\partial R}{\partial w_2} \\
                        \vdots \\
                        \frac{\partial R}{\partial w_n}
                        \end{bmatrix}
                        = \begin{bmatrix}
                        c_1 \\
                        c_2 \\
                        \vdots \\
                        c_n
                        \end{bmatrix} 
                        = C
                        \]
                    </p>
                    Now say,
                    <p style="text-align:center;"> 
                         $R = W^{\mathrm{T}} C = [\,w_1 \; w_2 \; \cdots \; w_n\,]
                        \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$</p>
                        then also,
                        \[
                        \frac{\partial R}{\partial w}
                        = C 
                        \] 
                    </p>
                    Hence, 
                    \[
                    \frac{\partial }{\partial w} (W^{\mathrm{T}} C) = \frac{\partial }{\partial w} (C^{\mathrm{T}} W) = C
                    \]
                    <br>
                    <p style="text-align:center;">
                        Consider $R = W^{\mathrm{T}} X^{\mathrm{T}} X W$. Assume $A = X^{\mathrm{T}} X$, then <br>
                        $\frac{\partial R}{\partial W} = AW + A^{\mathrm{T}} W = 2AW = 2X^{\mathrm{T}} X W$ (since $A$ is symmetric).
                    </p>



                </div>

                <div class="solution">
                    <h4>Solution:</h4>

                    <div class="step">
                        <strong>Part 1: Proving the relationship between tanh and sigmoid</strong>
                        <p>We know that the logistic function:</p>
                        \[
                        \sigma(a) = \frac{1}{1 + e^{-a}}
                        \]
                        <p>and the RHS can be written as</p>
                        \[
                        2 . \frac{1}{1 + e^{-2a}} - 1 => \frac{2e^{2a}}{1 + e^{2a}} - 1
                        \]
                        <p>solving the above equation gives us</p>
                        \[
                        \frac{e^{2a} - 1}{e^{2a} + 1}
                        \]
                        <p>which is nothing but \(\tanh(a)\) (a different form though)</p>
                    </div>

                    <div class="step">
                        <strong>Part 2: Converting sigmoid combination to tanh combination</strong>
                        <p>We can rewrite the relationship as:</p>
                        \[
                        \sigma(a) = \frac{\tanh\left(\frac{a}{2}\right) + 1}{2}
                        \]
                        <p>Substitute this relation into equation (1.101):</p>
                        \[
                        \begin{aligned}
                        y(x,w) &= w_0 + \sum_{j=1}^M w_j\,\sigma\!\left(\frac{x-\mu_j}{s}\right) \\
                        &= w_0 + \sum_{j=1}^M w_j\cdot\frac{1}{2}\left[\tanh\!\left(\frac{x-\mu_j}{2s}\right) + 1\right]
                        \\
                        &= \left(w_0 + \frac{1}{2}\sum_{j=1}^M w_j\right) + \sum_{j=1}^M
                        \frac{w_j}{2}\,\tanh\!\left(\frac{x-\mu_j}{2s}\right)
                        \end{aligned}
                        \]
                        <p>which aligns to eqn (1.102) (not sure if there's a printing mistake but you get the idea)</p>
                    </div>

                    <div class="note">
                        <strong>üí° Key Insight:</strong> The sigmoid and the hyperbolic tangent are affine transforms of
                        each other (up to a scale change in the argument), so linear combinations of one family can
                        always be expressed as linear combinations of the other by simple rescaling of parameters. This
                        shows that neural networks using sigmoid vs tanh activations are fundamentally equivalent in
                        their representational power.
                    </div>
                </div>
            </div>

            <!-- Template for next problem -->
            

        <footer>
            <p><a href="../index.html">Home</a> | <a href="chapter2.html">Next Chapter ‚Üí</a></p>
            <p style="margin-top: 10px;">Created with ‚ù§Ô∏è for the ML community</p>
        </footer>
    </div>
</body>

</html>