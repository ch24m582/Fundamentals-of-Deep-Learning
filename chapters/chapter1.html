<!-- chapter1.html -->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Basics Needed for Deep Learning</title>

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="../js/math-config.js"></script>
</head>

<body>
    <div class="container">
        <a href="../index.html" class="back-link">← Back to Home</a>

        <header>
            <h1>Chapter 1: Basics Needed for Deep Learning</h1>
        </header>

        <div class="chapter-intro">
            <p>
                \[
                y = h(x) = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2
                \]
                <img src="../assets/LinearForm.png" alt="Linear Regression Illustration" class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy"-->
                - Guess the form of the hypothesis function for a problem with N features.<br>
                - here the form is linear combination of the features and their interactions.<br>
                - we will fix the form of the hypothesis function and learn the parameters (weights) from data.
                
            </p>
        </div>

        <div class="Gradient-Descent">
            <div class="Gradient-Descent">
                <h2>Gradient-Descent</h2>

                <div class="Gradient-Descent Algorithm">
                    <h4>Gradient Descent Update Rule</h4>
                    <p>
                    \[
                    w^{(t+1)} = w^t - \alpha \nabla_w J(w^t)
                    \]
                    <h4>Stoppping criteria:</h4>
                    \[
                    ||w^{(t+1)} - w^t|| &equiv; || \nabla_w J(w^t) || &equiv; || J(W^{(t+1)} - J(w^t)) || < \epsilon
                    \]
                    </p>
                </div>

                <div class ="Gradient Descent for Linear Regression">
                    <h4>Gradient Descent for Linear Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x$ &nbsp;&nbsp;  (We always assume m as total samples)
                    </p>
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(\hat{y}^{(i)} - y^{(i)}\big)^2\]
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(w_0 + w_1x^{(i)} - y^{(i)}\big)^2\]
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} \]
                        \[w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} \]
                    
                    </p>

                    <h4>Gradient Descent for Quadratic Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x + w_2 x^2$ 
                    </p>
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[\frac{\partial J}{\partial w_2} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).{x^{(i)}}^2\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_2 = w_2 - \alpha\frac{\partial J}{\partial w_2} \]
                    
                    </p>   
                    <h4>How a typical data matrix and y looks like</h4>
                    <p> 
                        \[
                        \begin{bmatrix}
                        1 & x_1^{(1)} & x_2^{(1)} & \ldots & x_n^1 \\
                        1 & x_1^{(2)} & x_2^{(2)} & \ldots & x_n^2 \\
                        \vdots & \vdots  & \ddots & \vdots \\
                        1 & x_1^{(m)} & x_2^{(m)} & \ldots & x_n^m 
                        \end{bmatrix}
                        \begin{bmatrix}
                        w_0 \\
                        w_1 \\
                        \vdots \\
                        w_n
                        \end{bmatrix}
                        =
                        \begin{bmatrix}
                        y^{(1)} \\
                        y^{(2)} \\
                        \vdots \\
                        y^{(m)}
                        \end{bmatrix}
                        \]
                    </p>
                    <p style="text-align:center;"> 
                      A typical input can be as weird as shown below:
                      \[x_i^m = sin(\omega x_j^m) &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = log(\omega x_j^m)  &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = \exp\left(-\frac{(x_j^{(m)} - \mu_j)^2}{2\sigma_j^2}\right) \]
                      Weights will remain linear but the input can be as weired 
                    </p>


                </div>

                <div class="note">
                    <h4>Notes</h4>
                    <p>
                        \[ J(w) =\frac{1}{2m} ||XW-Y ||^2 \]
                        Let us say
                        <p style="text-align:center;">$R = C^{\mathrm{T}} W = [\,c_1 \; c_2 \; \cdots \; c_n\,]
                        \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$</p>
                        where \(C\) is a constant vector independent of \(W\). Then,
                        \[
                        \frac{\partial R}{\partial w}
                        = \begin{bmatrix}
                        \frac{\partial R}{\partial w_1} \\
                        \frac{\partial R}{\partial w_2} \\
                        \vdots \\
                        \frac{\partial R}{\partial w_n}
                        \end{bmatrix}
                        = \begin{bmatrix}
                        c_1 \\
                        c_2 \\
                        \vdots \\
                        c_n
                        \end{bmatrix} 
                        = C
                        \]
                    </p>
                    Now say,
                    <p style="text-align:center;"> 
                         $R = W^{\mathrm{T}} C = [\,w_1 \; w_2 \; \cdots \; w_n\,]
                        \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$</p>
                        then also,
                        \[
                        \frac{\partial R}{\partial w}
                        = C 
                        \] 
                    </p>
                    Hence, 
                    \[
                    \frac{\partial }{\partial w} (W^{\mathrm{T}} C) = \frac{\partial }{\partial w} (C^{\mathrm{T}} W) = C
                    \]
                    <br>
                    <p style="text-align:center;">
                        Consider $R = W^{\mathrm{T}} X^{\mathrm{T}} X W$. Assume $A = X^{\mathrm{T}} X$, then <br>
                        $\frac{\partial R}{\partial W} = AW + A^{\mathrm{T}} W = 2AW = 2X^{\mathrm{T}} X W$ (since $A$ is symmetric).
                    </p>
                    <p>
                        <br>Let us use above tricks to derive closed form of w
                        \[ J = ||XW - Y||^2\]
                        \[J = (XW - Y)^{\mathrm{T}}(XW - Y) = W^{\mathrm{T}}X^{\mathrm{T}}XW - 2Y^{\mathrm{T}}XW + Y^{\mathrm{T}}Y \]
                        \[\frac{\partial J}{\partial W} = 2X^{\mathrm{T}}XW - 2X^{\mathrm{T}}Y \]
                        Setting $\frac{\partial J}{\partial W} = 0$, we get
                        \[X^{\mathrm{T}}XW = X^{\mathrm{T}}Y \]
                        \[W = (X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}Y \]
                    
                    </p>



                </div>

                <div style="margin:20px; padding:10px; border:1px solid #ddd; border-radius:6px;">
                    <h3 style="margin-top:0;">Batch Gradient Descent Notebook</h3>
                    <p>
                        Explore and run the gradient descent code interactively in Google Colab.<br>
                        <a href="https://colab.research.google.com/github/ch24m582/Fundamentals-of-Deep-Learning/blob/main/codes/BatchGradientDescent.ipynb" target="_blank" style="font-weight:bold; color:#0066cc; text-decoration:none;">
                        Batch Gradient Descent Notebook in Colab
                        </a>
                    </p>
                </div>
            </div>

        <!-- Template for next problem -->
        <div class="Logistic Regression">
            <h2>From Linear to Logistic Regression</h2>
            <p><strong>Linear Regression:</strong></p>
            <p>
                \[y = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n\]
            </p>
            <p><strong>Logistic Regression:</strong></p>
            <p>
                \[\hat{y} = \sigma\!\big(w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n\big),\quad \sigma(z) = \frac{1}{1+e^{-z}}\]
            </p>
            <p><strong>Q)</strong> Why do we need logistic regression when we already have linear regression?</p>

            <p><strong>Q)</strong> What is the issue with using linear regression for classification problems, and how does Binary Cross Entropy solve it?</p>

            <p>
                \[ J(y,\hat{y}) = (y - \hat{y})^2 \] 
                -> Does not give strong feedback as shown in below example<br>
                -> Comes natural from gauussian likelihood perspective   
            </p>
            <p><strong>Example:</strong></p>
            Let us take the example of patient with cancer detection.<br>
            Let us say the true label is \(y=1\) (patient has cancer). and \(y=0\) (patient does not have cancer).<br>
            Now, let us see how the squared error loss behaves for different predicted values \(\hat{y}\):
            <ul>
                <li>
                    Let us say \[ \hat{y} = 0.9\] and person does not have cancer so 
                    \[J = (0 - 0.9)^2 = 0.81\]
                    <ul>
                    <li>The model gets very less penalty for being wrong</li>
                    <li>Cost is also bounded by 1</li>
                    </ul>
                </li>
            </ul>
            <p>
                <img src="../assets/BCE_issues.png" 
                        alt="BCE issues" 
                        class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" 
                        loading="lazy">
            </p>

            <p>
                so, derive a new cost function for classidfication problems called Binary Cross Entropy (BCE) loss.<br>
                \[ J(y,\hat{y}) = -y \log(\hat{y}) - (1-y) \log(1-\hat{y}) \]
            </p>
            <p>
                Example showing it is a better loss function:<br>
                Eg: Let us say that Truth is cancer and \(y = 1\), and \(\hat{y}\) = 0.8<br>
                \[ J_{BCE} = -1 \log(0.8) - (1-1) \log(1-0.8) = -\log(0.8) \approx 0.223 \]<br>
                Now, let us say that Truth is no cancer and \(y = 0\), and \(\hat{y} \approx 0  or \epsilon\)<br>
                \[ J_{BCE} = - (1-0) \log(1-\epsilon) = -\log(0.8) \approx -\log(1) -> 0 \]
                Assume \(y =1\) and \(\hat{y} = 0.0\)<br>
                the cost will be very high as \(-\log(0) -> \infty\)

                <br> More confidently you make mistake, the loss function will peanlize more
            </p>
        <div>  
            <h3>Deriving Gradient for Logistic Regression with BCE loss</h3>  
            <p>
                Gradient Setup of logistic regression with BCE loss:<br>
                <img src="../assets/log_gradient_setup.png" 
                        alt="log_gradient_setup" 
                        class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" 
                        loading="lazy">
                \[J = -y \ln(\hat{y}) - (1-y) \ln(1-\hat{y})   \]
                \[\hat{y} = \sigma(z) = \frac{1}{1+e^{-z}} \]
                \[z = w^{\mathrm{T}} x = w_0 + w_1 x_1  \]
                \[ \frac{\partial J}{\partial w_0} = \frac{\partial J}{\partial \hat{y} } \frac{\partial \hat{y}}{\partial {z} } \frac{\partial z}{\partial {w_0} }
                ,&nbsp;&nbsp; \frac{\partial J}{\partial w_1} = \frac{\partial J}{\partial \hat{y} } \frac{\partial \hat{y}}{\partial {z} } \frac{\partial z}{\partial {w_1} } \]
                
                \[\frac{\partial J}{\partial \hat{y} } = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})} \]
                \[\frac{\partial \hat{y}}{\partial {z} } = \left(\frac{e^{-z}}{1+e^{-z}}\right) \left(\frac{1}{1+e^{-z}}\right) =\hat{y}(1-\hat{y}) \]
                \[\frac{\partial z}{\partial {w_0} } = 1 ,&nbsp;&nbsp; \frac{\partial z}{\partial {w_1} } = x_1 \]
                \[\therefore \frac{\partial J}{\partial w_0} = (\hat{y} - y).1 ,&nbsp;&nbsp; \frac{\partial J}{\partial w_1} = (\hat{y} - y).x_1 \]
                The bound that was created using exponential function in the sigmoid is again brought back to normal form using logarithm.<br>
                sigmoid created bound: \(0 < \hat{y} < 1\)<br>
                logarithm removed the bound: \(-\infty < J < \infty\)

                \[
                J_{BCE} = \left\{ \begin{array}{ll}
                -\ln(\hat{y}) & \text{if } y=1 \\
                -\ln(1-\hat{y}) & \text{if } y=0
                \end{array} \right.
                \]
                \[J_{BCE} = -y \ln(\hat{y}) - (1-y) \ln(1-\hat{y})   \]
            </p>

            <p>
                <div style="margin:20px; padding:10px; border:1px solid #ddd; border-radius:6px;">
                    <h3 style="margin-top:0;">Logistic Gradient Descent Notebook</h3>
                    <p>
                        Explore and run the Logistic gradient descent code interactively in Google Colab.<br>
                        <a href="https://colab.research.google.com/github/ch24m582/Fundamentals-of-Deep-Learning/blob/main/codes/LogisticGradDescent.ipynb" target="_blank" style="font-weight:bold; color:#0066cc; text-decoration:none;">
                        Logistic Gradient Descent Notebook in Colab
                        </a>
                    </p>
                </div>
            </p>
        </div>
        
        <div>
            <h3>Multiclass Classification -> Categorical Cross Entropy(CCE)</h3>
            <p>
                For multiclass classification problems, we use Categorical Cross Entropy (CCE) loss function.<br>
                \[ J_{CCE} = - \sum_{i=1}^{K} y_i \log(\hat{y}_i) \]
                where \(K\) is the number of classes, \(y_i\) is the true label.<br>
                The softmax function is used to convert logits into probabilities for each class.<br>
                \[\hat{y}_i = \frac{e^{z_i}}{\sum_{k=1}^{K} e^{z_k}} \]
                where \(z_i\) is the logit for class \(i\).
            </p>

        </div>

        <div>
            <h3>Scalar Chain with no bias</h3>
            <p>
                <img src="../assets/scalarchain.png" 
                        alt="log_gradient_setup" 
                        class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" 
                        loading="lazy">
            </p>
            <p>
                \[x = a0 \quad  z1 = a_0 w_1 \quad z2 = w_2 a_1 \quad z3 = w_3 a_2 \]
                \[\quad \quad \quad \quad a_1 = g(z_1) \quad a_2 = g(z_2) \quad a_3 = g(z_3) \]
                Finite Difference Method to compute gradients:<br>
                \[\frac{\partial J}{\partial w_3} = \frac{J(w_3 + \Delta w) - J(w_3)}{\Delta w} \]
                \[\frac{\partial J}{\partial w_2} = \frac{J(w_2 + \Delta w) - J(w_2)}{\Delta w} \]
                \[\frac{\partial J}{\partial w_1} = \frac{J(w_1 + \Delta w) - J(w_1)}{\Delta w} \]
                Change \(w_3\) from 1 to 1.0001 and see the change in J to compute \(\frac{\partial J}{\partial w_3}\)<br>
                similarly for \(w_2\) and \(w_1\).<br><br>
                To estimate gradient using FDM is computationally expensive as we need to do forward pass for each weight perturbation.<br>
                <b>Solution</b>: Use Backpropagation to compute gradients efficiently using chain rule.
                </p>

                <p>
                    Simple Neural Network
                    <img src="../assets/basicnn.png" 
                        alt="log_gradient_setup" 
                        class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" 
                        loading="lazy">
                </p>

        </div>







            
        </div>    

        <footer>
            <p><a href="../index.html">Home</a> | <a href="chapter2.html">Next Chapter →</a></p>
            <p style="margin-top: 10px;">Created with ❤️ for the ML community</p>
        </footer>
    </div>
</body>

</html>