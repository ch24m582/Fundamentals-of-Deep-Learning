<!-- chapter1.html -->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Basics Needed for Deep Learning</title>

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="../js/math-config.js"></script>
</head>

<body>
    <div class="container">
        <a href="../index.html" class="back-link">← Back to Home</a>

        <header>
            <h1>Chapter 1: Basics Needed for Deep Learning</h1>
        </header>

        <div class="chapter-intro">
            <p>
                \[
                y = h(x) = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2
                \]
                <img src="../assets/LinearForm.png" alt="Linear Regression Illustration" class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy"-->
                - Guess the form of the hypothesis function for a problem with N features.<br>
                - here the form is linear combination of the features and their interactions.<br>
                - we will fix the form of the hypothesis function and learn the parameters (weights) from data.
                
            </p>
        </div>

        <div class="Gradient-Descent">
            <div class="Gradient-Descent">
                <h2>Gradient-Descent</h2>

                <div class="Gradient-Descent Algorithm">
                    <h4>Gradient Descent Update Rule</h4>
                    <p>
                    \[
                    w^{(t+1)} = w^t - \alpha \nabla_w J(w^t)
                    \]
                    <h4>Stoppping criteria:</h4>
                    \[
                    ||w^{(t+1)} - w^t|| &equiv; || \nabla_w J(w^t) || &equiv; || J(W^{(t+1)} - J(w^t)) || < \epsilon
                    \]
                    </p>
                </div>

                <div class ="Gradient Descent for Linear Regression">
                    <h4>Gradient Descent for Linear Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x$ &nbsp;&nbsp;  (We always assume m as total samples)
                    </p>
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(\hat{y}^{(i)} - y^{(i)}\big)^2\]
                        \[J(w_0,w_1) = \frac{1}{2m} \sum_{i=1}^{m} \big(w_0 + w_1x^{(i)} - y^{(i)}\big)^2\]
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} \]
                        \[w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} \]
                    
                    </p>

                    <h4>Gradient Descent for Quadratic Regression</h4>
                    <p>
                    <p style="text-align:center;">
                        $y = w_0 + w_1x + w_2 x^2$ 
                    </p>
                        \[\frac{\partial J}{\partial w_0} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).1\]
                        \[\frac{\partial J}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).x^{(i)}\]
                        \[\frac{\partial J}{\partial w_2} = \frac{1}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big).{x^{(i)}}^2\]
                        \[w_0 = w_0 - \alpha\frac{\partial J}{\partial w_0} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_1 = w_1 - \alpha\frac{\partial J}{\partial w_1} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        w_2 = w_2 - \alpha\frac{\partial J}{\partial w_2} \]
                    
                    </p>   
                    <h4>How a typical data matrix and y looks like</h4>
                    <p> 
                        \[
                        \begin{bmatrix}
                        1 & x_1^{(1)} & x_2^{(1)} & \ldots & x_n^1 \\
                        1 & x_1^{(2)} & x_2^{(2)} & \ldots & x_n^2 \\
                        \vdots & \vdots  & \ddots & \vdots \\
                        1 & x_1^{(m)} & x_2^{(m)} & \ldots & x_n^m 
                        \end{bmatrix}
                        \begin{bmatrix}
                        w_0 \\
                        w_1 \\
                        \vdots \\
                        w_n
                        \end{bmatrix}
                        =
                        \begin{bmatrix}
                        y^{(1)} \\
                        y^{(2)} \\
                        \vdots \\
                        y^{(m)}
                        \end{bmatrix}
                        \]
                    </p>
                    <p style="text-align:center;"> 
                      A typical input can be as weird as shown below:
                      \[x_i^m = sin(\omega x_j^m) &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = log(\omega x_j^m)  &nbsp;&nbsp;or&nbsp;&nbsp; x_i^m = \exp\left(-\frac{(x_j^{(m)} - \mu_j)^2}{2\sigma_j^2}\right) \]
                      Weights will remain linear but the input can be as weired 
                    </p>


                </div>

                <div class="note">
                    <h4>Notes</h4>
                    <p>
                        \[ J(w) =\frac{1}{2m} ||XW-Y ||^2 \]
                        Let us say
                        <p style="text-align:center;">$R = C^{\mathrm{T}} W = [\,c_1 \; c_2 \; \cdots \; c_n\,]
                        \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}$</p>
                        where \(C\) is a constant vector independent of \(W\). Then,
                        \[
                        \frac{\partial R}{\partial w}
                        = \begin{bmatrix}
                        \frac{\partial R}{\partial w_1} \\
                        \frac{\partial R}{\partial w_2} \\
                        \vdots \\
                        \frac{\partial R}{\partial w_n}
                        \end{bmatrix}
                        = \begin{bmatrix}
                        c_1 \\
                        c_2 \\
                        \vdots \\
                        c_n
                        \end{bmatrix} 
                        = C
                        \]
                    </p>
                    Now say,
                    <p style="text-align:center;"> 
                         $R = W^{\mathrm{T}} C = [\,w_1 \; w_2 \; \cdots \; w_n\,]
                        \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$</p>
                        then also,
                        \[
                        \frac{\partial R}{\partial w}
                        = C 
                        \] 
                    </p>
                    Hence, 
                    \[
                    \frac{\partial }{\partial w} (W^{\mathrm{T}} C) = \frac{\partial }{\partial w} (C^{\mathrm{T}} W) = C
                    \]
                    <br>
                    <p style="text-align:center;">
                        Consider $R = W^{\mathrm{T}} X^{\mathrm{T}} X W$. Assume $A = X^{\mathrm{T}} X$, then <br>
                        $\frac{\partial R}{\partial W} = AW + A^{\mathrm{T}} W = 2AW = 2X^{\mathrm{T}} X W$ (since $A$ is symmetric).
                    </p>
                </div>

                <div style="margin:20px; padding:10px; border:1px solid #ddd; border-radius:6px;">
                    <h3 style="margin-top:0;">Batch Gradient Descent Notebook</h3>
                    <p>
                        Explore and run the gradient descent code interactively in Google Colab.<br>
                        <a href="https://colab.research.google.com/github/ch24m582/Fundamentals-of-Deep-Learning/blob/main/codes/BatchGradientDescent.ipynb" target="_blank" style="font-weight:bold; color:#0066cc; text-decoration:none;">
                        Batch Gradient Descent Notebook in Colab
                        </a>
                    </p>
                    
                    <p>
                        Logistic Gradient Descent
                    </p>






                </div>


            </div>

            <!-- Template for next problem -->
            

        <footer>
            <p><a href="../index.html">Home</a> | <a href="chapter2.html">Next Chapter →</a></p>
            <p style="margin-top: 10px;">Created with ❤️ for the ML community</p>
        </footer>
    </div>
</body>

</html>